library("FactoMineR")
library("factoextra")
## Générer la matrice user - tag
dataset <- matrix(sample.int(99, size = 20*26, replace = TRUE),nr=20,nc=26)
## Génération des rows name
row.names(dataset) <- c(sprintf("user%s",seq(1:20)))
## Génération des tags name pour les colonnes
`colnames<-`(dataset, LETTERS[seq( from = 1, to = 26 )])
## Clustering Kmeans
kmeansWithTwoClusters <- kmeans(dataset, 3, nstart = 20)
kmeansWithSixClusters <- kmeans(dataset, 6, nstart = 20)
# Tableau d'apartenance des utilisateur au différent cluster
kmeansWithTwoClusters$cluster
kmeansWithSixClusters$cluster
library(data.table)
library(ggplot2)
library(ggmap)
library(mclust)
library("FactoMineR")
library("factoextra")
## Générer la matrice user - tag
dataset <- matrix(sample.int(99, size = 20*26, replace = TRUE),nr=20,nc=26)
## Génération des rows name
row.names(dataset) <- c(sprintf("user%s",seq(1:20)))
## Génération des tags name pour les colonnes
`colnames<-`(dataset, LETTERS[seq( from = 1, to = 26 )])
## Clustering Kmeans
kmeansWithThreeClusters <- kmeans(dataset, 3, nstart = 20)
kmeansWithSixClusters <- kmeans(dataset, 6, nstart = 20)
# Tableau d'apartenance des utilisateur au différent cluster
kmeansWithThreeClusters$cluster
kmeansWithSixClusters$cluster
# Determine le nombre optimal de cluster différent pour le k-means clustering: QUESTION PROF MACHINE LEARNING: COMMENT LIRE CE GRAPH ?
fviz_nbclust(dataset, kmeans,method = "gap_stat")
# On visualise les clusters!
fviz_cluster(kmeansWithThreeClusters, data = dataset, ellipse.type = "convex", palette = "jco", repel = TRUE, ggtheme = theme_minimal())
fviz_cluster(kmeansWithSixClusters, data = dataset, ellipse.type = "convex", palette = "jco", repel = TRUE, ggtheme = theme_minimal())
View(dataset)
View(dataset)
## Génération des tags name pour les colonnes
`colnames<-`(dataset, LETTERS[seq( from = 1, to = 26 )])
View(dataset)
View(dataset)
## Générer la matrice user - tag
dataset <- matrix(sample.int(99, size = 20*26, replace = TRUE),nr=20,nc=26)
View(dataset)
View(dataset)
## Génération des rows name
row.names(dataset) <- c(sprintf("user%s",seq(1:20)))
## Génération des tags name pour les colonnes
`colnames<-`(dataset, LETTERS[seq( from = 1, to = 26 )])
# Determine le nombre optimal de cluster différent pour le k-means clustering
fviz_nbclust(dataset, kmeans,method = "gap_stat")
library(data.table)
library(ggplot2)
library(ggmap)
library(mclust)
library("FactoMineR")
library("factoextra")
# Determine le nombre optimal de cluster différent pour le k-means clustering
fviz_nbclust(dataset, kmeans,method = "gap_stat")
kmeansWithThreeClusters <- kmeans(dataset, 4, nstart = 20)
# Tableau d'apartenance des utilisateur au différent cluster
kmeansWithThreeClusters$cluster
# On visualise les clusters!
fviz_cluster(kmeansWithThreeClusters, data = dataset, ellipse.type = "convex", palette = "jco", repel = TRUE, ggtheme = theme_minimal())
library(twitteR)  ## cf. setup_twitter_oauth()
setup_twitter_oauth()
install.packages("twitteR")
setup_twitter_oauth()
install.packages("stringr")
install.packages("snippets")
tw = userTimeline("chlalanne", n = 1000)
setup_twitter_oauth()
install.packages("htr")
tw = userTimeline("chlalanne", n = 1000)
install.packages("httr")
library(httr)
tw = userTimeline("chlalanne", n = 1000)
library(twitteR)  ## cf. setup_twitter_oauth()
library(stringr)
library(snippets)
library(httr)
tw = userTimeline("chlalanne", n = 1000)
setup_twitter_oauth()
setup_twitter_oauth(GbUrKk3bFBD1irGvTaHvK9RK3,YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
)
s
setup_twitter_oauth(GbUrKk3bFBD1irGvTaHvK9RK3,YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
)
setup_twitter_oauth(GbUrKk3bFBD1irGvTaHvK9RK3,YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
)
setup_twitter_oauth(GbUrKk3bFBD1irGvTaHvK9RK3,YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
)
tw = userTimeline("chlalanne", n = 1000)
get_oauth_sig()
setup_twitter_oauth(GbUrKk3bFBD1irGvTaHvK9RK3,YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
,NULL,NULL)
setup_twitter_oauth("GbUrKk3bFBD1irGvTaHvK9RK3","YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
",NULL,NULL)
setup_twitter_oauth("GbUrKk3bFBD1irGvTaHvK9RK3","YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
")
tw = userTimeline("chlalanne", n = 1000)
setup_twitter_oauth("GbUrKk3bFBD1irGvTaHvK9RK3","YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
")
setup_twitter_oauth("GbUrKk3bFBD1irGvTaHvK9RK3","YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
")
setup_twitter_oauth("	943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","	hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
library(twitteR)  ## cf. setup_twitter_oauth()
library(stringr)
library(snippets)
install.packages("snippets")
install.packages("snippets")
library(httr)
setup_twitter_oauth("	943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","	hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
sessionInfo()
setup_twitter_oauth("	943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","	hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
library(twitteR)  ## cf. setup_twitter_oauth()
library(stringr)
library(snippets)
library(httr)
setup_twitter_oauth("943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
setup_twitter_oauth("943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
setup_twitter_oauth("943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
tw = userTimeline("chlalanne", n = 1000)
find.tag = function(x)
unlist(str_extract_all(x$getText(), "#[A-Za-z0-9]*"))
tags = lapply(tw, function(x) try(find.tag(x),
silent = TRUE))
sort(table(unlist(tags)), decr = TRUE)
wcl = table(unlist(tags))
names(wcl) = str_replace_all(names(wcl), "#", "")
cloud(wcl[wcl > 5])
setup_twitter_oauth("943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
setup_twitter_oauth("943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
library(twitteR)  ## cf. setup_twitter_oauth()
library(stringr)
library(snippets)
library(httr)
setup_twitter_oauth("943110616998400000-2OVCsMgSQzxcrBX1lRaKjn4XW3wgocj","hHyFx3l6UdkpbEHj7VoAQMqkiQudu9tnNZBWbIa0enJL3
")
setup_twitter_oauth("	GbUrKk3bFBD1irGvTaHvK9RK3","YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
")
setup_twitter_oauth("GbUrKk3bFBD1irGvTaHvK9RK3","YH3xToGsIBm6t9iUvUVfRLwfo0g92fmItTv5O88NgVIAY69Iuq
")
read.csv(bank-data.csv)
read.csv("bank-data.csv")
dt <- read.csv("bank-data.csv")
(
library(data.table)
dt <- read.csv("bank-data.csv")
library(data.table)
dt <- read.csv("bank-data.csv")
dt <- read.csv("C:\Users\rushi\Documents\R\bank-data.csv")
dt <- read.csv("C:/Users/rushi/Documents/R/bank-data.csv")
View(dt)
View(dt)
c1 <- cut(dt$age, breaks = 4)
c1 <- cut(dt$age, breaks = ({0-34,35-51,52-67}))
c1 <- cut(dt$age, breaks = ({0:34,35:51,52:67}))
c1 <- cut(dt$age, breaks = c({0:34,35:51,52:67}))
c1 <- cut(dt$age, breaks = c({"0:34","35:51","52:67"}))
c1 <- cut(dt$age, breaks = c("0:34","35:51","52:67"))
c1 <- cut(dt$age, breaks = c(0:34,35:51,52:67))
table(c1)
c1 <- cut(dt$age, breaks = c(0-34,35-51,52-67))
table(c1)
c1 <- cut(dt$age, breaks = (0-34,35-51,52-67))
c1 <- cut(dt$age, breaks = c(0,35,52))
table(c1)
c1 <- cut(dt$age, breaks = c([0-34],[35-51],[52-67]))
c1 <- cut(dt$age, breaks = ([0-34],[35-51],[52-67]))
c1 <- cut(dt$age, breaks = ([0,34],[35,51],[52,67]))
c1 <- cut(dt$age, breaks = c(0,35,51,67), include.lowest = TRUE)
table(c1)
c1 <- cut(dt$age, breaks = c(0,34,51,67), include.lowest = TRUE)
table(c1)
View(dt)
View(dt)
c2 <- cut(dt$income, breaks = c(0,24386,43758,63130))
table(c2)
# Partie 1:
#   Question 1:
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer-reviews.csv")
# Partie 1:
#   Question 1:
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
# Partie 1:
#   Question 1:
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
# Partie 1:
#   Question 1:
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
set.seed(12739)
sample(dtBear, 1586614/10, replace = FALSE, prob = NULL)
sample(length(dtBear), 1586614/10, replace = FALSE, prob = NULL)
sample(dtBear, 1586614/10, replace = FALSE, prob = NULL)
sample(1586614, 1586614/10, replace = FALSE, prob = NULL)
sample(dtBear, 1586614/10, replace = FALSE, prob = NULL)
dtBear[sample(1586614, 1586614/10, replace = FALSE, prob = NULL)]
echantillon <- dtBear[sample(1586614, 1586614/10, replace = FALSE, prob = NULL)]
echantillon <- dtBear[sample(1586614, 1586614/10, replace = FALSE, prob = NULL)]
install.packages("arules")
install.packages("arulesViz")
library("arules");
install.packages("arules")
library("arules", lib.loc="~/R/win-library/3.4")
library("arules");
rules = apriori(dt, parameter=list(support=0.1, confidence=0.5, lift=1.5));
rules = apriori(dt, parameter=list(support=0.1, confidence=0.5));
#   Question 2:
bankIncome = as(dt, "income");
rules = apriori(dt, parameter=list(support=0.1, confidence=0.5));
rules = apriori(dt$income, parameter=list(support=0.1, confidence=0.5));
#   Question 2:
bankIncome = as(dt$income, "income");
rules = apriori(dt$income, parameter=list(support=0.1, confidence=0.5));
#   Question 2:
bankIncome = as(dt$income, "transtactions");
#   Question 2:
bankIncome = as(dt, "transtactions");
rules = apriori(dt$income, parameter=list(support=0.1, confidence=0.5));
rules = apriori(dt, parameter=list(support=0.1, confidence=0.5));
# Partie 1
#   Question 1:
dt$c1 <- cut(dt$age, breaks = c(0,34,51,67), include.lowest = TRUE)
table(c1)
dt$c2 <- cut(dt$income, breaks = c(0,24386,43758,63130))
table(c2)
table(dt$c1)
table(dt$c2)
library(data.table)
library("arules");
dt <- read.csv("C:/Users/rushi/Documents/R/bank-data.csv")
# Partie 1
#   Question 1:
dt$c1 <- cut(dt$age, breaks = c(0,34,51,67), include.lowest = TRUE)
table(dt$c1)
dt$c2 <- cut(dt$income, breaks = c(0,24386,43758,63130))
table(dt$c2)
rules = apriori(dt, parameter=list(support=0.1, confidence=0.5));
View(dt)
View(dt)
rules = apriori(dt[2,5,7], parameter=list(support=0.1, confidence=0.5));
view(dt[2,5,7])
View(dt[2,5,7])
View(dt[c(2,5,7)])
rules = apriori(dt[c(2,5,7)], parameter=list(support=0.1, confidence=0.5));
rules = apriori(dt[c(1,3,4,6,8,9,10,11,12,13,14)], parameter=list(support=0.1, confidence=0.5));
View(rules)
View(rules)
rules@quality[["lift"]]
#rules@quality[["lift"]]
head(sort(rules, by="lift"),10)
#rules@quality[["lift"]]
listOfRules <- head(sort(rules, by="lift"),10)
View(listOfRules)
View(listOfRules)
View(listOfRules)
View(listOfRules)
# Partie 2:
#   Question 1:
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
set.seed(12739)
echantillon <- dtBear[sample(1586614, 1586614/10, replace = FALSE, prob = NULL)]
listOfRules
dte = sample(row.names(dte), 1586614, replace = FALSE)
dtBear = sample(row.names(dtBear), 1586614, replace = FALSE)
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
dtBearSample = sample(row.names(dtBear), 1586614, replace = FALSE)
set.seed(12739)
dtBearSample = sample(row.names(dtBear), 1586614, replace = FALSE)
dtBearSample = sample(row.names(dtBear), 1586614/10, replace = FALSE)
#   Question 1:
names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
pca <- prcomp(dtBearSample[,-1], center = TRUE, scale = TRUE)
pca <- prcomp(dtBearSample, center = TRUE, scale = TRUE)
names(dtBearSample)
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
set.seed(12739)
dtBearSample = sample(row.names(dtBear), 1586614/10, replace = FALSE)
dtBearSample <- sample(row.names(dtBear), 1586614/10, replace = FALSE)
inspect(listOfRules)
row.names(dtBear)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample, center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[,-1], center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[,1], center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[1,1], center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[1,], center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[-1,], center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[,0], center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[0], center = TRUE, scale = TRUE)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
#names(dtBearSample)
pca <- prcomp(dtBearSample[1], center = TRUE, scale = TRUE)
View(dtBear)
View(dtBear)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
dtBearSample[c(5,6,9,10)]
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
res <- dtBearSample[c(5,6,9,10)]
dtBearSample <- sample(dtBear, 1586614/10, replace = FALSE)
dtBearSample <- sample(length(dtBear), 1586614/10, replace = FALSE)
dtBearSample <- sample(length(dtBear)/10, 1586614, replace = FALSE)
dtBearSample <- sample(length(dtBear), 1586614/10, replace = FALSE)
length(dtBear)
length(row.names(dtBear))
dtBearSample <- sample(length(row.names(dtBear)), 1586614/10, replace = FALSE)
dtBearSample <- dtBear[sample( 1586614 , 1586614/10, replace = FALSE)]
dtBearSample <- dtBear(sample( 1586614 , 1586614/10, replace = FALSE))
dtBearSample <- dtBear.table(sample( 1586614 , 1586614/10, replace = FALSE))
dtBearSample <- data.table(sample( 1586614 , 1586614/10, replace = FALSE))
dtBearSample <- sample(row.names(dtBear), 1586614/10, replace = FALSE)
pca <- prcomp(dtBearSample, center = TRUE, scale = TRUE)
dtBearSample <- dtBear[sample(row.names(dtBear), 1586614/10, replace = FALSE),]
View(dtBearSample)
View(dtBearSample)
#   Question 1:
#names(dtBearSample) <- c("review_appearance", "review_aroma","review_palate","review_taste")
res <- dtBearSample[c(5,6,9,10)]
View(res)
View(res)
#res <- dtBearSample[c(5,6,9,10)]
pca <- prcomp(dtBearSample, center = TRUE, scale = TRUE)
#res <- dtBearSample[c(5,6,9,10)]
pca <- prcomp(dtBearSample, center = TRUE, scale = TRUE)
#res <- dtBearSample[c(5,6,9,10)]
pca <- prcomp(dtBearSample[c(5,6,9,10)], center = TRUE, scale = TRUE)
library("ggplot2")
library("ggfortify")
install.packages("ggfortify")
library("ggfortify")
autoplot(pca)
#   Question 2:
#bankIncome = as(dt, "transtactions");
#View(dt[c(2,5,7)])
#rules = apriori(dt[c(1,3,4,6,8:14)], parameter=list(support=0.1, confidence=0.5));
rules = apriori(dt[,-c(1.7)], parameter = list(support = 0.1))
inspect(rules)
#   Question 2:
#bankIncome = as(dt, "transtactions");
#View(dt[c(2,5,7)])
rules = apriori(dt[c(1,3,4,6,8:14)], parameter=list(support=0.1));
#rules@quality[["lift"]]
listOfRules <- head(sort(rules, by="lift"),10)
inspect(listOfRules)
autoplot(pca)
library(data.table)
library("arules")
library("ggplot2")
library("ggfortify")
library("arules", lib.loc="~/R/win-library/3.4")
detach("package:arules", unload=TRUE)
library("arules", lib.loc="~/R/win-library/3.4")
dt <- read.csv("C:/Users/rushi/Documents/R/bank-data.csv")
# Partie 1
#   Question 1:
dt$c1 <- cut(dt$age, breaks = c(0,34,51,67), include.lowest = TRUE)
table(dt$c1)
dt$c2 <- cut(dt$income, breaks = c(0,24386,43758,63130))
table(dt$c2)
#   Question 2:
#bankIncome = as(dt, "transtactions");
#View(dt[c(2,5,7)])
rules = apriori(dt[c(1,3,4,6,8:14)], parameter=list(support=0.1));
#rules@quality[["lift"]]
listOfRules <- head(sort(rules, by="lift"),10)
inspect(listOfRules)
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
set.seed(12739)
dtBearSample <- dtBear[sample(row.names(dtBear), 1586614/10, replace = FALSE),]
#res <- dtBearSample[c(5,6,9,10)]
pca <- prcomp(dtBearSample[c(5,6,9,10)], center = TRUE, scale = TRUE)
autoplot(pca)
library(factoextra)
#res <- dtBearSample[c(5,6,9,10)]
resultat.pca <- prcomp(dtBearSample[c(5,6,9,10)], center = TRUE, scale = TRUE)
library(data.table)
#library("arules")
library("arules", lib.loc="~/R/win-library/3.4")
library("ggplot2")
library("ggfortify")
library(factoextra)
dt <- read.csv("C:/Users/rushi/Documents/R/bank-data.csv")
# Partie 1
#   Question 1:
dt$c1 <- cut(dt$age, breaks = c(0,34,51,67), include.lowest = TRUE)
table(dt$c1)
dt$c2 <- cut(dt$income, breaks = c(0,24386,43758,63130))
table(dt$c2)
#   Question 2:
#bankIncome = as(dt, "transtactions");
#View(dt[c(2,5,7)])
rules = apriori(dt[c(1,3,4,6,8:14)], parameter=list(support=0.1));
#rules@quality[["lift"]]
listOfRules <- head(sort(rules, by="lift"),10)
inspect(listOfRules)
dtBear <- read.csv("C:/Users/rushi/Documents/R/beer_reviews.csv")
set.seed(12739)
dtBearSample <- dtBear[sample(row.names(dtBear), 1586614/10, replace = FALSE),]
#res <- dtBearSample[c(5,6,9,10)]
resultat.pca <- prcomp(dtBearSample[c(5,6,9,10)], center = TRUE, scale = TRUE)
autoplot(resultat.pca)
fviz_eig(resultat.pca)
fviz_pca_ind(resultat.pca,
col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
fviz_pca_ind(resultat.pca, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
fviz_pca_var(resultat.pca, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
# Graphique des variables corrélées positivement. On remarque qu'elles pointent tous sur la gauche du graphe (Negatif).
fviz_pca_var(resultat.pca, col.var = "contribution", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
# Graphique des variables corrélées positivement. On remarque qu'elles pointent tous sur la gauche du graphe (Negatif).
fviz_pca_var(resultat.pca, col.var = "contri", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
# Graphique des variables corrélées positivement. On remarque qu'elles pointent tous sur la gauche du graphe (Negatif).
fviz_pca_var(resultat.pca, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
# On montre le pourcentage de variances expliquées par chaque composante principale
fviz_eig(resultat.pca)
fviz_pca_ind(res.pca, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = FALSE)
fviz_pca_ind(resultat.pca, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = FALSE)
fviz_pca_ind(resultat.pca, col.ind = "cos1", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = FALSE)
# Score sur la première composante
fviz_pca_ind(resultat.pca, col.ind = x, gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = FALSE)
# Score sur la première composante
fviz_pca_ind(resultat.pca, col.ind = "x", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = FALSE)
# On commence par prendre un autre echantillon de 10%
dtBearSample2 <- dtBear[sample(row.names(dtBear), 1586614/10, replace = FALSE),]
View(dtBearSample)
View(dtBearSample)
View(dtBearSample2)
View(dtBearSample2)
dtBearSample2.coord <- predict(resultat.pca, newdata = dtBearSample2)
# Plot of active individuals
p <- fviz_pca_ind(resultat.pca, repel = TRUE)
# Add supplementary individuals
fviz_add(p, dtBearSample2.coord, color ="blue")
# Plot of active individuals
p <- fviz_pca_ind(resultat.pca, repel = FALSE)
# Add supplementary individuals
fviz_add(p, dtBearSample2.coord, color ="blue")
user_dedup <- read.csv("C:/Users/rushi/Desktop/PROJETR/user_dedup.json", header=FALSE)
library("rjson")
library(data.table)
library(recommenderlab)
#getwd()
setwd("C:/Users/rushi/Desktop/PROJETR/")
#links <- read.csv("C:/Users/rushi/Desktop/PROJETR/links.csv")
movies <- read.csv("C:/Users/rushi/Desktop/PROJETR/movies.csv")
ratings <- read.csv("C:/Users/rushi/Desktop/PROJETR/ratings.csv")
tags <- read.csv("C:/Users/rushi/Desktop/PROJETR/tags.csv")
mergeMovRat <- merge(ratings, movies, by="movieId")
mergeMovRat <- as(mergeMovRat, "realRatingMatrix")
#realAllMerged <- as(allmerged, "realRatingMatrix")
hist(getRatings(mergeMovRat), breaks = 20)
hist(getRatings(normalize(mergeMovRat)), breaks = 20)
evaluationSchema <-evaluationScheme(mergeMovRat,method='cross-validation',train=.8,given=1,goodRating=4,k=10)
ubcfRecommender <- Recommender(getData(evaluationSchema,"train"),"UBCF")
ibcfRecommender <- Recommender(getData(evaluationSchema,"train"),"IBCF")
svdRecommender <- Recommender(getData(evaluationSchema,"train"),"svd")
popularRecommender <- Recommender(getData(evaluationSchema,"train"),"POPULAR")
randomRecommender <- Recommender(getData(evaluationSchema,"train"),"RANDOM")
ubcfPrediction <- predict(ubcfRecommender,getData(evaluationSchema,"known"),type="ratings")
ibcfPrediction <- predict(ibcfRecommender,getData(evaluationSchema,"known"),type="ratings")
svdPrediction <- predict(svdRecommender,getData(evaluationSchema,"known"),type="ratings")
popPrediction <- predict(popularRecommender,getData(evaluationSchema,"known"),type="ratings")
randPrediction <- predict(randomRecommender,getData(evaluationSchema,"known"),type="ratings")
ubcfPredictionAccuracy <- calcPredictionAccuracy(ubcfPrediction,getData(evaluationSchema,"unknown"))
ibcfPredictionAccuracy <- calcPredictionAccuracy(ibcfPrediction,getData(evaluationSchema,"unknown"))
svdPredictionAccuracy <- calcPredictionAccuracy(svdPrediction,getData(evaluationSchema,"unknown"))
popPredictionAccuracy <- calcPredictionAccuracy(popPrediction,getData(evaluationSchema,"unknown"))
randPredictionAccuracy <- calcPredictionAccuracy(randPrediction,getData(evaluationSchema,"unknown"))
PredictionAccuracy <- rbind(ubcfPredictionAccuracy,ibcfPredictionAccuracy,svdPredictionAccuracy,popPredictionAccuracy,randPredictionAccuracy)
rownames(PredictionAccuracy) <- c("UBCF","IBCF","SVD","POP","RAND")
View(PredictionAccuracy)
View(PredictionAccuracy)
